{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Mask_Detector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "T7I8LuQX__lX",
        "4BLSZwPa0bay",
        "eoI-QTAvhFZb",
        "lz4Ufn7Mh8F6",
        "HJMd5ueStpAD",
        "90ncuwUhZ_R3",
        "GOHXl0Psch24",
        "6Xku-U92ElSc"
      ],
      "mount_file_id": "1Dwg5uspIgQ4zRWQFuSeQOJTjYxVXA04V",
      "authorship_tag": "ABX9TyMGauiR6/4XQFU/mm91S9SY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erengursu/Face_Mask_Detector/blob/main/Face_Mask_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7aarQ2qzjIe"
      },
      "source": [
        "# **Face Mask Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E6YpLl6UmNt"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpR3kTslHXS8"
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExRW-ckRVne"
      },
      "source": [
        "**Mounting GDrive:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJD2M0lm-HIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30647ef8-e739-4e79-aa22-2bffe8117a58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3YkZgTPQaJO"
      },
      "source": [
        "**Remove Folder with its subfolders:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkKAmiUgigMB"
      },
      "source": [
        "!rm -rf dataset/"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQz5PTRQjmw"
      },
      "source": [
        "**Creates compressed ZIP file from the folder:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4C6dvK8VdUI"
      },
      "source": [
        "!zip -r dataset_shuffled.zip dataset_shuffled/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0psVsPAP_a3"
      },
      "source": [
        "**IndÄ±cates how many file under the folder:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWBxu8RaY79"
      },
      "source": [
        "!ls /content/dataset_shuffled_v2/without_mask/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81-lO_rMQGz0"
      },
      "source": [
        "**Total size of the folders' subcontent:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQSincMZc-kJ"
      },
      "source": [
        "!du -h /content/imagens/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_x3n0pldNPY"
      },
      "source": [
        "!ls -lsh /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh_Nnk5S9IDI"
      },
      "source": [
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "mdl = os.path.sep.join([os.getcwd(), \"drive\", \"MyDrive\", \"avatar.jpg\"])\n",
        "print(mdl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMKGda7T0HGu"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD2KIJTFJFcB"
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zaJEkLrJQIS"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EMEHrkmgUiw"
      },
      "source": [
        "!rm -rf imagens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c2uXTt6RJK4"
      },
      "source": [
        "**Uploading from the local:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdcrE3HrIwcU",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "d6bd3988-aba0-4160-a3d9-280b7bcbc581"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd0dac1e-6418-4a56-b5de-1043d5d89155\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd0dac1e-6418-4a56-b5de-1043d5d89155\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbRTg6LqQ8MC"
      },
      "source": [
        "**Extracting the .zip compressed file:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypgh3O0kNOc5"
      },
      "source": [
        "!unzip /content/drive/MyDrive/mask_dataset_images.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Ngbuw7fxhs"
      },
      "source": [
        "!mv /content/with_mask_improper /content/dataset_shuffled_v2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq7Yd2McQyS6"
      },
      "source": [
        "**Listing of the content of the folder:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrjRLE7zNm2j"
      },
      "source": [
        "!ls /content/dataset_shuffled/without_mask/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SvU44aBQ-ek"
      },
      "source": [
        "### Rename the files in directory\n",
        "\n",
        "Below shell command code rename the files as numbers in incremental fashion in current directory. Be aware that one first needs to go to intended directory first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugZCZ74UNgsI"
      },
      "source": [
        "ls -1prt | grep -v \"/$\" | cat -n | while read n f; do mv -n \"${f}\" \"$(printf \"%04d\" $n).${f#*.}\"; done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwJpHeAOsI4"
      },
      "source": [
        "### Shuffle from large dataset into small data set randomly\n",
        "\n",
        "Shuffle and copy specific number of item of specific file type from one directory to another randomly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH2nltN3PiBe"
      },
      "source": [
        "Shell command to copy 2000 .jpg type file randomly from dirst indicated directory to second indicated directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXe_W7YHLBsj"
      },
      "source": [
        "!find /content/dataset/with_mask/ -type f -name \"*.jpg\" -print0 | xargs -0 shuf -e -n2000 -z | xargs -0 cp -vt /content/dataset_shuffled/with_mask/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89rEmzCbP1Vc"
      },
      "source": [
        "Python and numpy script to shuffle and copy specified type of file(.jpg) with specified numbers from one directory to another. In this script, labels are in same directory with data images as .txt files. Therefore, that part is unneccessary to shuffle and copy purpose, so be aware!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12piEqDXQNFy"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "#set directories\n",
        "directory = str('/content/dataset_shuffled/with_mask_improper')\n",
        "target_directory = str('/content/dataset_shuffled_v2/with_mask_improper')\n",
        "data_set_percent_size = float(0.07)\n",
        "\n",
        "#print(os.listdir(directory))\n",
        "\n",
        "# list all files in dir that are an image\n",
        "files = [f for f in os.listdir(directory) if f.endswith('.jpg')]\n",
        "\n",
        "#print(files)\n",
        "\n",
        "# select a percent of the files randomly, or like we did give specific number \n",
        "random_files = random.sample(files, int(2000))\n",
        "#random_files = random.sample(files, int(len(files)*data_set_percent_size))\n",
        "#random_files = np.random.choice(files, int(len(files)*data_set_percent_size))\n",
        "\n",
        "#print(random_files)\n",
        "\n",
        "# move the randomly selected images by renaming directory \n",
        "\n",
        "for random_file_name in random_files:      \n",
        "    #print(directory+'/'+random_file_name)\n",
        "    #print(target_directory+'/'+random_file_name)\n",
        "    os.rename(directory+'/'+random_file_name, target_directory+'/'+random_file_name)\n",
        "    continue\n",
        "\n",
        "# move the relevant labels for the randomly selected images\n",
        "\n",
        "#for image_labels in random_files:\n",
        "    # strip extension and add .txt to find corellating label file then rename directory. \n",
        "    #os.rename(directory+'/'+(os.path.splitext(image_labels)[0]+'.txt'), target_directory+'/'+(os.path.splitext(image_labels)[0]+'.txt'))\n",
        "\n",
        "    #continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BSjWv4W0Qvd"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1WZiLgbRrmT"
      },
      "source": [
        "Train from the shell command prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdYdwWPPJbvU"
      },
      "source": [
        "!python train_mask_detector_3class.py --dataset dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1batQc2Z53qF"
      },
      "source": [
        "### Face Detector Trainer Script\n",
        "\n",
        "This code is suitable to be used inline command as script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUDUEa6Z0wd_"
      },
      "source": [
        "# USAGE\n",
        "#Original python script. Intended to be used in line.\n",
        "# python train_mask_detector.py --dataset dataset\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
        "\thelp=\"path to dataset\")\n",
        "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "\thelp=\"path to output loss/accuracy plot\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"face_mask_detector.model\",\n",
        "\thelp=\"path to output face mask detector model\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BS = 32\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the input image (224x224) and preprocess it\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = preprocess_input(image)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "# convert the data and labels to NumPy arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "\n",
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving mask detector model...\")\n",
        "model.save(args[\"model\"], save_format=\"h5\")\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(args[\"plot\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxSIIdUT6K9V"
      },
      "source": [
        "### Face Mask Trainer Colab Script\n",
        "\n",
        "This script is suitable to be run on Colab. One must careful about default locations fiven for face_detector scripts. For our case:\n",
        "\n",
        "'/content/dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMHxMAo2LlX0"
      },
      "source": [
        "#### Working Colab script for 2 classes : with_mask and without_mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAYVR0F2LVSg"
      },
      "source": [
        "# USAGE\n",
        "# Suitable for Colab usage\n",
        "# One must be careful about dataset destination\n",
        "# USAGE FROM TERMINAL\n",
        "# python train_mask_detector.py --dataset \"dataset_folder_name\"\n",
        "# Note that script must be in the same directory with your dataset folder.\n",
        "\n",
        "### Changelog ####\n",
        "# Confusion_matrix calculation added to last line\n",
        "# Dataset: \"content/imagens\" with 3 class\n",
        "# Batch_size: 32\n",
        "# EPOCH : 20\n",
        "# Dense.relu : 128\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow is a binary categorical variable with values 0 and 1 w.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BS = 32\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images('/content/dataset_shuffled_v2/'))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the input image (224x224) and preprocess it\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = preprocess_input(image)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "# convert the data and labels to NumPy arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "\n",
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "# Dense softmax layer is made to be 3 to accomodate 3 classes\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(3, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving mask detector model...\")\n",
        "model.save('/content/face_mask_detector.model', save_format=\"h5\")\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig('/content/plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRYcYvx5LR_E"
      },
      "source": [
        "#### Colab script for 3 classes : with_mask, with_mask_improper and without_mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847yjuYm7F2j"
      },
      "source": [
        "# USAGE\n",
        "# Suitable for Colab usage\n",
        "# One must be careful about dataset destination\n",
        "# USAGE FROM TERMINAL\n",
        "# python train_mask_detector.py --dataset \"dataset_folder_name\"\n",
        "# Note that script must be in the same directory with your dataset folder.\n",
        "\n",
        "### Changelog ####\n",
        "# Confusion_matrix calculation added to last line\n",
        "# Dataset: \"content/imagens\" with 3 class\n",
        "# Batch_size: 32\n",
        "# EPOCH : 20\n",
        "# Dense.relu : 128 (Might be 256 with whole 'imagens' dataset)\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BS = 32\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images('/content/dataset_shuffled_v2/'))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the input image (224x224) and preprocess it\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = preprocess_input(image)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "# convert the data and labels to NumPy arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "\n",
        "# perform one-hot encoding on the labels\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "with_mask_improper\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),with_mask_improper\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving mask detector model...\")\n",
        "model.save('/content/face_mask_detector.model', save_format=\"h5\")\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig('/content/plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIUJGeFFdgI"
      },
      "source": [
        "#### Experimental Colab script for 3 classes : with_mask, with_mask_improper and without_mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7DJK39vBYDa",
        "outputId": "5b0a2307-4a8b-42d7-e342-fa0332fd7d36"
      },
      "source": [
        "# USAGE\n",
        "# Suitable for Colab usage\n",
        "# One must be careful about dataset destination\n",
        "\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BS = 32\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images('/content/dataset_shuffled_v2/'))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t# load the input image (224x224) and preprocess it\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = preprocess_input(image)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "# convert the data and labels to NumPy arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWn_aSIZC8oa"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBT8nXoOCHYG"
      },
      "source": [
        "# perform one-hot encoding on the labels\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoTOhKsWDMB1"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3mIFN1YsBfTA",
        "outputId": "83acefcf-c52b-4027-db3d-2bb60f62dc14"
      },
      "source": [
        "\n",
        "# partition the data into trainin)g and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(3, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=le.classes_))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving mask detector model...\")\n",
        "model.save('/content/face_mask_detector.model', save_format=\"h5\")\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")with_mask_improper\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig('/content/plot.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "[INFO] compiling model...\n",
            "[INFO] training head...\n",
            "Epoch 1/20\n",
            "150/150 [==============================] - 83s 338ms/step - loss: 0.4524 - accuracy: 0.7038 - val_loss: 0.1197 - val_accuracy: 0.9725\n",
            "Epoch 2/20\n",
            "150/150 [==============================] - 49s 327ms/step - loss: 0.1413 - accuracy: 0.9466 - val_loss: 0.0680 - val_accuracy: 0.9775\n",
            "Epoch 3/20\n",
            "150/150 [==============================] - 49s 326ms/step - loss: 0.0893 - accuracy: 0.9688 - val_loss: 0.0494 - val_accuracy: 0.9833\n",
            "Epoch 4/20\n",
            "150/150 [==============================] - 49s 329ms/step - loss: 0.0709 - accuracy: 0.9709 - val_loss: 0.0392 - val_accuracy: 0.9883\n",
            "Epoch 5/20\n",
            "150/150 [==============================] - 49s 329ms/step - loss: 0.0656 - accuracy: 0.9716 - val_loss: 0.0339 - val_accuracy: 0.9867\n",
            "Epoch 6/20\n",
            "150/150 [==============================] - 49s 328ms/step - loss: 0.0593 - accuracy: 0.9763 - val_loss: 0.0300 - val_accuracy: 0.9900\n",
            "Epoch 7/20\n",
            "150/150 [==============================] - 49s 327ms/step - loss: 0.0485 - accuracy: 0.9807 - val_loss: 0.0274 - val_accuracy: 0.9917\n",
            "Epoch 8/20\n",
            "150/150 [==============================] - 49s 325ms/step - loss: 0.0444 - accuracy: 0.9816 - val_loss: 0.0253 - val_accuracy: 0.9933\n",
            "Epoch 9/20\n",
            "150/150 [==============================] - 49s 326ms/step - loss: 0.0374 - accuracy: 0.9843 - val_loss: 0.0233 - val_accuracy: 0.9917\n",
            "Epoch 10/20\n",
            "150/150 [==============================] - 49s 327ms/step - loss: 0.0376 - accuracy: 0.9835 - val_loss: 0.0218 - val_accuracy: 0.9925\n",
            "Epoch 11/20\n",
            "150/150 [==============================] - 49s 325ms/step - loss: 0.0299 - accuracy: 0.9888 - val_loss: 0.0210 - val_accuracy: 0.9908\n",
            "Epoch 12/20\n",
            "150/150 [==============================] - 49s 328ms/step - loss: 0.0339 - accuracy: 0.9856 - val_loss: 0.0193 - val_accuracy: 0.9917\n",
            "Epoch 13/20\n",
            "150/150 [==============================] - 49s 325ms/step - loss: 0.0276 - accuracy: 0.9892 - val_loss: 0.0200 - val_accuracy: 0.9925\n",
            "Epoch 14/20\n",
            "150/150 [==============================] - 49s 327ms/step - loss: 0.0324 - accuracy: 0.9854 - val_loss: 0.0179 - val_accuracy: 0.9908\n",
            "Epoch 15/20\n",
            "150/150 [==============================] - 49s 328ms/step - loss: 0.0247 - accuracy: 0.9892 - val_loss: 0.0195 - val_accuracy: 0.9925\n",
            "Epoch 16/20\n",
            "150/150 [==============================] - 49s 326ms/step - loss: 0.0289 - accuracy: 0.9888 - val_loss: 0.0173 - val_accuracy: 0.9925\n",
            "Epoch 17/20\n",
            "150/150 [==============================] - 49s 328ms/step - loss: 0.0226 - accuracy: 0.9896 - val_loss: 0.0170 - val_accuracy: 0.9933\n",
            "Epoch 18/20\n",
            "150/150 [==============================] - 49s 325ms/step - loss: 0.0212 - accuracy: 0.9884 - val_loss: 0.0190 - val_accuracy: 0.9925\n",
            "Epoch 19/20\n",
            "150/150 [==============================] - 49s 325ms/step - loss: 0.0282 - accuracy: 0.9859 - val_loss: 0.0164 - val_accuracy: 0.9917\n",
            "Epoch 20/20\n",
            "150/150 [==============================] - 49s 327ms/step - loss: 0.0248 - accuracy: 0.9897 - val_loss: 0.0160 - val_accuracy: 0.9925\n",
            "[INFO] evaluating network...\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "         with_mask       0.99      0.99      0.99       400\n",
            "with_mask_improper       0.99      0.99      0.99       400\n",
            "      without_mask       1.00      1.00      1.00       400\n",
            "\n",
            "          accuracy                           0.99      1200\n",
            "         macro avg       0.99      0.99      0.99      1200\n",
            "      weighted avg       0.99      0.99      0.99      1200\n",
            "\n",
            "[INFO] saving mask detector model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7I8LuQX__lX"
      },
      "source": [
        "### Another approach from internet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p39OOFeWAFHr"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# Params\n",
        "IMAGE_WIDTH = 300\n",
        "IMAGE_HEIGHT = 300\n",
        "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS = 1\n",
        "IMG_DIR = '/content/dataset/'\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 4\n",
        "# Model\n",
        "base_model = MobileNetV2(\n",
        "    weights= None,\n",
        "    include_top=False,\n",
        "    input_shape= (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256,activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "opt = Adam(lr=0.000125)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6E6J34wAmbZ"
      },
      "source": [
        "callbacks_list = [ModelCheckpoint('../weights/service_weights.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'),\n",
        "    EarlyStopping(monitor='val_accuracy', patience=5),\n",
        "    ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIvwoqjqAqnv"
      },
      "source": [
        "def add_noise(img):\n",
        "    '''Add random noise to an image'''\n",
        "    VARIABILITY = 8\n",
        "    deviation = VARIABILITY*random.random()\n",
        "    noise = np.random.normal(0, deviation, img.shape)\n",
        "    img += noise\n",
        "    np.clip(img, 0., 255.)    return imgtrain_datagen = ImageDataGenerator(\n",
        "    brightness_range=[0.2, 1.6],\n",
        "    rescale=1. / 255,\n",
        "    rotation_range=0,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\",\n",
        "    preprocessing_function=add_noise,\n",
        ")train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    IMG_DIR,\n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    color_mode = 'grayscale',\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)validation_generator = validation_datagen.flow_from_dataframe(\n",
        "    validate_df,\n",
        "    IMG_DIR,\n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    color_mode = 'grayscale',\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uEw6XQlAyUM"
      },
      "source": [
        "epochs=50 if FAST_RUN else 50history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=total_validate//BATCH_SIZE,\n",
        "    steps_per_epoch=total_train//BATCH_SIZE,\n",
        "    callbacks=callbacks_list\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaQeSv1RBmIP"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(validate_df.category, validate_df.pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BLSZwPa0bay"
      },
      "source": [
        "## **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5rdFxJ5QpBv"
      },
      "source": [
        "!unzip file.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkNZDu7eRDjX"
      },
      "source": [
        "!rm file.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYGaJxnPPWEo"
      },
      "source": [
        "!rm detect_mask_video_v3.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tzo0RNd2_Nv"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWxJMERAQicG"
      },
      "source": [
        "!python detect_mask_video_v3.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yKQbQylRfoQ"
      },
      "source": [
        "!python detect_mask_image_v6.py --image test_imageset/without_mask/nm_02.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZKc0aIQflYs"
      },
      "source": [
        ">### ***Make Directory Where All Our Captured Videos are stored***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki5Jq5mIgvkz"
      },
      "source": [
        "!git clone https://github.com/opencv/opencv.git\n",
        "!mkdir Video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoI-QTAvhFZb"
      },
      "source": [
        "\n",
        "\n",
        ">### ***Import Javascript Code to Open Webcam***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liiv8UxyhbLO"
      },
      "source": [
        "!pip install ffmpeg-python\n",
        "\n",
        "\n",
        "from IPython.display import HTML, Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "video_file_test = '/content/Video/osy_test.mp4' \n",
        "  \n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var my_btn_txt = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(my_btn_txt);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, videoStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  videoStream = stream;\n",
        "  var options = {  \n",
        "    mimeType : 'video/webm;codecs=vp9'  \n",
        "  };            \n",
        "  recorder = new MediaRecorder(stream, options);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('video');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "    \n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({video: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      videoStream.getVideoTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... Please wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def start_webcam():\n",
        "  js = Javascript('''\n",
        "    async function startWebcam() {\n",
        "      const div = document.createElement('div');\n",
        "      \n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      \n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      \n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      \n",
        "      return;\n",
        "    \n",
        "    }\n",
        "    ''')\n",
        "  \n",
        "  display(js)\n",
        "  data = eval_js('startWebcam()')\n",
        "  \n",
        "    \n",
        "start_webcam()\n",
        "\n",
        "def get_video():\n",
        "  display(HTML(VIDEO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  return binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz4Ufn7Mh8F6"
      },
      "source": [
        ">### ***Open Captured Video***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9LxE-1-kV-v"
      },
      "source": [
        "videofile = get_video()\n",
        "with open(video_file_test, 'wb') as f:\n",
        "  f.write(videofile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJMd5ueStpAD"
      },
      "source": [
        ">### ***Some Example Code using Captured Video Stream***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpQetoyDtpiE"
      },
      "source": [
        "import cv2  \n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('/content/opencv/data/haarcascades/haarcascade_frontalface_default.xml') \n",
        "eye_cascade = cv2.CascadeClassifier('/content/opencv/data/haarcascades/haarcascade_eye.xml')  \n",
        "\n",
        "cap = cv2.VideoCapture(video_file_test) \n",
        "  \n",
        "count = 1\n",
        "while cap.isOpened() & count<30:\n",
        "  \n",
        "    ret, img = cap.read()  \n",
        "  \n",
        "   # gray = cv2.cvtColor(img, cv2.COLOR_BGR2BGR) \n",
        "  \n",
        "    # Detects faces of different sizes in the input image \n",
        "    faces = face_cascade.detectMultiScale(img, 1.3, 5) \n",
        "  \n",
        "    for (x,y,w,h) in faces: \n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,255,0),2)  \n",
        "       # roi_gray = gray[y:y+h, x:x+w] \n",
        "        roi_color = img[y:y+h, x:x+w] \n",
        "  \n",
        "        eyes = eye_cascade.detectMultiScale(roi_color)  \n",
        "  \n",
        "        for (ex,ey,ew,eh) in eyes: \n",
        "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,127,255),2) \n",
        "\n",
        "    cv2_imshow(img) \n",
        "  \n",
        "    k = cv2.waitKey(30) & 0xff\n",
        "    if k == 27: \n",
        "        break\n",
        "  \n",
        "cap.release() \n",
        "cv2.destroyAllWindows()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ncuwUhZ_R3"
      },
      "source": [
        "### **Detect Mask From Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbuZ-MpJao4m"
      },
      "source": [
        "\n",
        "#version == 6 >> Inline detection result return added << \n",
        "\n",
        "# USAGE\n",
        "# python detect_mask_image.py --image test_imageset/with_mask/m_01.jpg\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#from skimage import io\n",
        "#from PIL import Image\n",
        "#import matplotlib.pylab as plt\n",
        "import os\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"path to input image\")\n",
        "ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "\tdefault=\"face_detector\",\n",
        "\thelp=\"path to face detector model directory\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"mask_detector.model\",\n",
        "\thelp=\"path to trained face mask detector model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "model = load_model(args[\"model\"])\n",
        "\n",
        "# load the input image from disk, clone it, and grab the image spatial\n",
        "# dimensions\n",
        "image = cv2.imread(args[\"image\"])\n",
        "orig = image.copy()\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "# construct a blob from the image\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0))\n",
        "\n",
        "# pass the blob through the network and obtain the face detections\n",
        "print(\"[INFO] computing face detections...\")\n",
        "net.setInput(blob)\n",
        "detections = net.forward()\n",
        "\n",
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# extract the confidence (i.e., probability) associated with\n",
        "\t# the detection\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t# filter out weak detections by ensuring the confidence is\n",
        "\t# greater than the minimum confidence\n",
        "\tif confidence > args[\"confidence\"]:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t# the object\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t# the frame\n",
        "\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\tface = img_to_array(face)\n",
        "\t\tface = preprocess_input(face)\n",
        "\t\tface = np.expand_dims(face, axis=0)\n",
        "\n",
        "\t\t# pass the face through the model to determine if the face\n",
        "\t\t# has a mask or not\n",
        "\t\t(mask, withoutMask) = model.predict(face)[0]\n",
        "\t\t\n",
        "\t\t# this is the section where I put the inline verbose. 'Twas last resort on GColab. Comment it out when running the script locally.\n",
        "\t\tdetection = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tconfidence_percentage = \"{}: {:.2f}%\".format(detection, max(mask, withoutMask) * 100)\n",
        "\t\tprint(confidence_percentage)\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\t# COMMENT IN WHEN DEPLOYING LOCALLY\n",
        "\t\t# label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\t# color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\t# label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\t# cv2.putText(image, label, (startX, startY - 10),\n",
        "\t\t\t#cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\t#cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "# show the output image\n",
        "#cv2_imshow(image)\n",
        "#cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv2cPDiwdnwD"
      },
      "source": [
        "\n",
        "# Version == 3 >> Intended to be used on Colab or Jupyter. cv2.imshow is replaced with cv2_imshow which doesn't need x server to operate <<\n",
        "\n",
        "# USAGE\n",
        "# python detect_mask_image.py --image test_imageset/with_mask/m_01.jpg\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow\n",
        "#from IPython.display import Image\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"path to input image\")\n",
        "ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "\tdefault=\"face_detector\",\n",
        "\thelp=\"path to face detector model directory\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"mask_detector.model\",\n",
        "\thelp=\"path to trained face mask detector model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = os.path.sep.join([\"/content/face_detector\", \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([\"/content/face_detector\",\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "model = load_model('/content/mask_detector.model')\n",
        "\n",
        "# load the input image from disk, clone it, and grab the image spatial\n",
        "# dimensions\n",
        "image = cv2.imread('/content/test_imageset/with_mask/m_01.jpg')\n",
        "orig = image.copy()\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "# construct a blob from the image\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0))\n",
        "\n",
        "# pass the blob through the network and obtain the face detections\n",
        "print(\"[INFO] computing face detections...\")\n",
        "net.setInput(blob)\n",
        "detections = net.forward()\n",
        "\n",
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# extract the confidence (i.e., probability) associated with\n",
        "\t# the detection\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t# filter out weak detections by ensuring the confidence is\n",
        "\t# greater than the minimum confidence\n",
        "\tif confidence > args[\"confidence\"]:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t# the object\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t# the frame\n",
        "\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\tface = img_to_array(face)\n",
        "\t\tface = preprocess_input(face)\n",
        "\t\tface = np.expand_dims(face, axis=0)\n",
        "\n",
        "\t\t# pass the face through the model to determine if the face\n",
        "\t\t# has a mask or not\n",
        "\t\t(mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(image, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "# show the output image\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdd05cz-c187"
      },
      "source": [
        "\n",
        "# Version == 1 >> Original Code. Can be used locally with x server system. <<\n",
        "\n",
        "# USAGE\n",
        "# python detect_mask_image.py --image test_imageset/with_mask/m_01.png\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\",required=True\n",
        "  help=\"path to input image\")\n",
        "ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "\tdefault=\"face_detector\",\n",
        "\thelp=\"path to face detector model directory\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"mask_detector.model\",\n",
        "\thelp=\"path to trained face mask detector model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "model = load_model(args[\"model\"])\n",
        "\n",
        "# load the input image from disk, clone it, and grab the image spatial\n",
        "# dimensions\n",
        "image = cv2.imread(args[\"image\"])\n",
        "orig = image.copy()\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "# construct a blob from the image\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0))\n",
        "\n",
        "# pass the blob through the network and obtain the face detections\n",
        "print(\"[INFO] computing face detections...\")\n",
        "net.setInput(blob)\n",
        "detections = net.forward()\n",
        "\n",
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# extract the confidence (i.e., probability) associated with\n",
        "\t# the detection\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t# filter out weak detections by ensuring the confidence is\n",
        "\t# greater than the minimum confidence\n",
        "\tif confidence > args[\"confidence\"]:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t# the object\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t# the frame\n",
        "\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\tface = img_to_array(face)\n",
        "\t\tface = preprocess_input(face)\n",
        "\t\tface = np.expand_dims(face, axis=0)\n",
        "\n",
        "\t\t# pass the face through the model to determine if the face\n",
        "\t\t# has a mask or not\n",
        "\t\t(mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(image, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "# show the output image\n",
        "cv2.imshow(\"Output\", image)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFMJyDwGehnw"
      },
      "source": [
        "!python detect_mask_image_v3.py --image /content/group_masked/eray.jpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOHXl0Psch24"
      },
      "source": [
        "### **Detect Mask From Webcam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4RnncSOekGR"
      },
      "source": [
        "\n",
        "# Version == 1 >> Original Code. Intended to be used on local machines. <<\n",
        "\n",
        "# USAGE\n",
        "# python detect_mask_video.py\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\n",
        "\t# initialize our list of faces, their corresponding locations,\n",
        "\t# and the list of predictions from our face mask network\n",
        "\tfaces = []\n",
        "\tlocs = []\n",
        "\tpreds = []\n",
        "\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the detection\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t\t# the frame\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\n",
        "\t\t\t# add the face and bounding boxes to their respective\n",
        "\t\t\t# lists\n",
        "\t\t\tfaces.append(face)\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\n",
        "\n",
        "\t# only make a predictions if at least one face was detected\n",
        "\tif len(faces) > 0:\n",
        "\t\t# for faster inference we'll make batch predictions on *all*\n",
        "\t\t# faces at the same time rather than one-by-one predictions\n",
        "\t\t# in the above `for` loop\n",
        "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
        "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "\t# return a 2-tuple of the face locations and their corresponding\n",
        "\t# locations\n",
        "\treturn (locs, preds)\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "\tdefault=\"face_detector\",\n",
        "\thelp=\"path to face detector model directory\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"mask_detector.model\",\n",
        "\thelp=\"path to trained face mask detector model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "maskNet = load_model(args[\"model\"])\n",
        "\n",
        "# initialize the video stream and allow the camera sensor to warm up\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 400 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=400)\n",
        "\n",
        "\t# detect faces in the frame and determine if they are wearing a\n",
        "\t# face mask or not\n",
        "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
        "\n",
        "\t# loop over the detected face locations and their corresponding\n",
        "\t# locations\n",
        "\tfor (box, pred) in zip(locs, preds):\n",
        "\t\t# unpack the bounding box and predictions\n",
        "\t\t(startX, startY, endX, endY) = box\n",
        "\t\t(mask, withoutMask) = pred\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# show the output frame\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve1UBCmtptSJ"
      },
      "source": [
        "> ### ***Combination of JS WebCam capture and live Face Detection Algorithm***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEzf9yg3wxBD"
      },
      "source": [
        "!git clone https://github.com/opencv/opencv.git\n",
        "!mkdir Video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ABIBJ_5wyG4"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y70CLKn2V2Lq"
      },
      "source": [
        "!python face_mask_detector_with_webcam_from_colab.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S34TSEWlqRE5"
      },
      "source": [
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "from IPython.display import HTML, Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # Used for Colab or Jupyter Environment\n",
        "import os\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "# Capturing  local WebCam using JS\n",
        "\n",
        "video_file_test = '/content/Video/osy_test.mp4' \n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var my_btn_txt = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(my_btn_txt);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, videoStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  videoStream = stream;\n",
        "  var options = {  \n",
        "    mimeType : 'video/webm;codecs=vp9'  \n",
        "  };            \n",
        "  recorder = new MediaRecorder(stream, options);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('video');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "    \n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({video: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      videoStream.getVideoTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... Please wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def start_webcam():\n",
        "  js = Javascript('''\n",
        "    async function startWebcam() {\n",
        "      const div = document.createElement('div');\n",
        "      \n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      \n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      \n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      \n",
        "      return;\n",
        "    \n",
        "    }\n",
        "    ''')\n",
        "  \n",
        "  display(js)\n",
        "  data = eval_js('startWebcam()')\n",
        "  \n",
        "    \n",
        "start_webcam()\n",
        "\n",
        "def get_video():\n",
        "  display(HTML(VIDEO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  return binary\n",
        "\n",
        "# Pipelining Captured WebCam feed to be used by Detection Algorithm\n",
        " \n",
        "  #videofile = get_video()\n",
        "#with open(video_file_test, 'wb') as f:\n",
        "  #f.write(videofile)\n",
        "\n",
        "# Detection module starts\n",
        "# Original code is modified by utilizing cv_imshaw(<image>) instead of cv2.imshaw() and\n",
        "# cap() function to use captured video instead of VideoStream and vs.read employing local video stream by webcam.\n",
        "\n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\n",
        "\t# initialize our list of faces, their corresponding locations,\n",
        "\t# and the list of predictions from our face mask network\n",
        "\tfaces = []\n",
        "\tlocs = []\n",
        "\tpreds = []\n",
        "\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the detection\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t\t# the frame\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\n",
        "\t\t\t# add the face and bounding boxes to their respective\n",
        "\t\t\t# lists\n",
        "\t\t\tfaces.append(face)\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\n",
        "\n",
        "\t# only make a predictions if at least one face was detected\n",
        "\tif len(faces) > 0:\n",
        "\t\t# for faster inference we'll make batch predictions on *all*\n",
        "\t\t# faces at the same time rather than one-by-one predictions\n",
        "\t\t# in the above `for` loop\n",
        "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
        "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "\t# return a 2-tuple of the face locations and their corresponding\n",
        "\t# locations\n",
        "\treturn (locs, preds)\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "\tdefault=\"face_detector\",\n",
        "\thelp=\"path to face detector model directory\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"mask_detector.model\",\n",
        "\thelp=\"path to trained face mask detector model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "maskNet = load_model(args[\"model\"])\n",
        "\n",
        "# initialize the video stream and allow the camera sensor to warm up\n",
        "# VideoCapture(<video>) is utilized instead of VideoStream()\n",
        "print(\"[INFO] starting video stream...\")\n",
        "#vs = VideoStream(src=0).start()\n",
        "vs = cv2.VideoCapture(video_file_test)\n",
        "#time.sleep(2.0)\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 400 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=400)\n",
        "\n",
        "\t# detect faces in the frame and determine if they are wearing a\n",
        "\t# face mask or not\n",
        "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
        "\n",
        "\t# loop over the detected face locations and their corresponding\n",
        "\t# locations\n",
        "\tfor (box, pred) in zip(locs, preds):\n",
        "\t\t# unpack the bounding box and predictions\n",
        "\t\t(startX, startY, endX, endY) = box\n",
        "\t\t(mask, withoutMask) = pred\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# show the output frame\n",
        "  # cv2_imshow(<image>) instead cv2.imshow() is utilized here\n",
        "\tcv2_imshow(frame)\n",
        "  #cv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\n",
        "# do a bit of cleanup\n",
        "# vs[streamed] is switched with vs[captured]\n",
        "vs.release()\n",
        "cv2.destroyAllWindows()\n",
        "#vs.stop()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqtgpqre2hn7"
      },
      "source": [
        "### **Second Take on Face Detection on Video Stream  WebCam JS pass through** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbtP_u-E3He3",
        "outputId": "6db3b267-480b-4324-ad8f-341dbb25ceae"
      },
      "source": [
        "# USAGE\n",
        "# python detect_mask_video.py\n",
        " \n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        " \n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "    # grab the dimensions of the frame and then construct a blob\n",
        "    # from it\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "        (104.0, 177.0, 123.0))\n",
        " \n",
        "    # pass the blob through the network and obtain the face detections\n",
        "    faceNet.setInput(blob)\n",
        "    detections = faceNet.forward()\n",
        " \n",
        "    # initialize our list of faces, their corresponding locations,\n",
        "    # and the list of predictions from our face mask network\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        " \n",
        "    # loop over the detections\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        # extract the confidence (i.e., probability) associated with\n",
        "        # the detection\n",
        "        confidence = detections[0, 0, i, 2]\n",
        " \n",
        "        # filter out weak detections by ensuring the confidence is\n",
        "        # greater than the minimum confidence\n",
        "        if confidence > 0.5:\n",
        "            # compute the (x, y)-coordinates of the bounding box for\n",
        "            # the object\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        " \n",
        "            # ensure the bounding boxes fall within the dimensions of\n",
        "            # the frame\n",
        "            (startX, startY) = (max(0, startX), max(0, startY))\n",
        "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        " \n",
        "            # extract the face ROI, convert it from BGR to RGB channel\n",
        "            # ordering, resize it to 224x224, and preprocess it\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        " \n",
        "            # add the face and bounding boxes to their respective\n",
        "            # lists\n",
        "            faces.append(face)\n",
        "            locs.append((startX, startY, endX, endY))\n",
        " \n",
        "    # only make a predictions if at least one face was detected\n",
        "    if len(faces) > 0:\n",
        "        # for faster inference we'll make batch predictions on *all*\n",
        "        # faces at the same time rather than one-by-one predictions\n",
        "        # in the above `for` loop\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = maskNet.predict(faces, batch_size=32)\n",
        " \n",
        "    # return a 2-tuple of the face locations and their corresponding\n",
        "    # locations\n",
        "    return (locs, preds)\n",
        " \n",
        "# load the face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "faceNet=cv2.dnn.readNet('/content/face_detector/deploy.prototxt','/content/face_detector/res10_300x300_ssd_iter_140000.caffemodel')\n",
        " \n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "maskNet = load_model('/content/face_mask_detector.model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading face detector model...\n",
            "[INFO] loading face mask detector model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghf4CmcEhdy-"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1-KQOd24h55"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        " \n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        " \n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        " \n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVdVEy3y4kld"
      },
      "source": [
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        " \n",
        "    return image_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUJwxN1x4o5Q"
      },
      "source": [
        "import imutils\n",
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "from google.colab.patches import cv2_imshow\n",
        "while True:\n",
        "  js_reply = take_photo(label_html, img_data)\n",
        "  if not js_reply:\n",
        "    break\n",
        "    \n",
        "  image = js_reply_to_image(js_reply)\n",
        " \n",
        "    # grab the frame from the threaded video stream and resize it\n",
        "    # to have a maximum width of 400 pixels\n",
        "  frame = image\n",
        "  v=True\n",
        "  if v == True:\n",
        " \n",
        "    frame = imutils.resize(frame, width=400)\n",
        " \n",
        "    # detect faces in the frame and determine if they are wearing a\n",
        "    # face mask or not\n",
        "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
        "    for (box, pred) in zip(locs, preds):\n",
        " \n",
        " \n",
        "        # unpack the bounding box and predictions\n",
        "      (startX, startY, endX, endY) = box\n",
        "      (mask, maskImproper, withoutMask) = pred\n",
        " \n",
        "        # determine the class label and color we'll use to draw\n",
        "        # the bounding box and text\n",
        "      if mask > withoutMask :\n",
        "        if mask > maskImproper :\n",
        "          label = \"Mask\"\n",
        "        else :\n",
        "          label = \"Improper Mask\"\n",
        "      elif withoutMask > maskImproper :\n",
        "        label = \"No Mask\"\n",
        "      else :\n",
        "        label = \"Improper Mask\"\n",
        "        \n",
        "      #label = \"Mask\" if mask > withoutMask and mask > maskImproper else \"No Mask\"\n",
        "        \n",
        "      if label == \"Mask\" :\n",
        "        color = (0, 255, 0)\n",
        "      elif label == \"Improper Mask\" :\n",
        "        color = (0, 127, 127)\n",
        "      else :\n",
        "        color = (0, 0, 255)\n",
        "      \n",
        "      #color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        " \n",
        "        # include the probability in the label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, maskImproper, withoutMask) * 100)\n",
        " \n",
        "        # display the label and bounding box rectangle on the output\n",
        "        # frame\n",
        "      frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      frame=cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        " \n",
        "    # show the output frame\n",
        "      cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xku-U92ElSc"
      },
      "source": [
        "# EXPERIMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sM1HRoTEqq7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTfjGb49EvMQ"
      },
      "source": [
        "# USAGE\n",
        "# python train_mask_detector.py --dataset dataset\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BS = 32\n",
        "\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images('/content/dataset_v2'))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "\n",
        "\t# extract the class label from the filename\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\t# load the input image (224x224) and preprocess it\n",
        "\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = preprocess_input(image)\n",
        "\n",
        "\t# update the data and labels lists, respectively\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "# convert the data and labels to NumPy arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "\n",
        "# perform one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=20,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
        "# left off\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# NOT be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKLftdj_E-FB"
      },
      "source": [
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FkoGRQgFLeI"
      },
      "source": [
        "## Testing via ImageSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmfZAUCjFDcX"
      },
      "source": [
        "#NOTE TO CHANGE DESTINATION OF IMAGE FOR MASK DETECTION\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# load the input image from disk, clone it, and grab the image spatial\n",
        "# dimensions\n",
        "image = cv2.imread('/content/group_maskedv2/mert.png')\n",
        "orig = image.copy()\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "# construct a blob from the image\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0))\n",
        "\n",
        "net=cv2.dnn.readNet('/content/face_detector/deploy.prototxt','/content/face_detector/res10_300x300_ssd_iter_140000.caffemodel')\n",
        "\n",
        "# pass the blob through the network and obtain the face detections\n",
        "print(\"[INFO] computing face detections...\")\n",
        "net.setInput(blob)\n",
        "detections = net.forward()\n",
        "\n",
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# extract the confidence (i.e., probability) associated with\n",
        "\t# the detection\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t# filter out weak detections by ensuring the confidence is\n",
        "\t# greater than the minimum confidence\n",
        "\tif confidence > 0.5:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t# the object\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t# the frame\n",
        "\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\tface = img_to_array(face)\n",
        "\t\tface = preprocess_input(face)\n",
        "\t\tface = np.expand_dims(face, axis=0)\n",
        "\n",
        "\t\t# pass the face through the model to determine if the face\n",
        "\t\t# has a mask or not\n",
        "\t\t(mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(image, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "# show the output image\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qqFsI4CFmnP"
      },
      "source": [
        "## Testing Using Webcam via JS pass-through"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcMtYzMJFx9g"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        " \n",
        "def detect_and_predict_mask(frame, faceNet, model):\n",
        "    # grab the dimensions of the frame and then construct a blob\n",
        "    # from it\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "        (104.0, 177.0, 123.0))\n",
        " \n",
        "    # pass the blob through the network and obtain the face detections\n",
        "    faceNet.setInput(blob)\n",
        "    detections = faceNet.forward()\n",
        " \n",
        "    # initialize our list of faces, their corresponding locations,\n",
        "    # and the list of predictions from our face mask network\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        " \n",
        "    # loop over the detections\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        # extract the confidence (i.e., probability) associated with\n",
        "        # the detection\n",
        "        confidence = detections[0, 0, i, 2]\n",
        " \n",
        "        # filter out weak detections by ensuring the confidence is\n",
        "        # greater than the minimum confidence\n",
        "        if confidence > 0.5:\n",
        "            # compute the (x, y)-coordinates of the bounding box for\n",
        "            # the object\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        " \n",
        "            # ensure the bounding boxes fall within the dimensions of\n",
        "            # the frame\n",
        "            (startX, startY) = (max(0, startX), max(0, startY))\n",
        "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        " \n",
        "            # extract the face ROI, convert it from BGR to RGB channel\n",
        "            # ordering, resize it to 224x224, and preprocess it\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        " \n",
        "            # add the face and bounding boxes to their respective\n",
        "            # lists\n",
        "            faces.append(face)\n",
        "            locs.append((startX, startY, endX, endY))\n",
        " \n",
        "    # only make a predictions if at least one face was detected\n",
        "    if len(faces) > 0:\n",
        "        # for faster inference we'll make batch predictions on *all*\n",
        "        # faces at the same time rather than one-by-one predictions\n",
        "        # in the above `for` loop\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = model.predict(faces, batch_size=32)\n",
        " \n",
        "    # return a 2-tuple of the face locations and their corresponding\n",
        "    # locations\n",
        "    return (locs, preds)\n",
        " \n",
        "faceNet=cv2.dnn.readNet('/content/face_detector/deploy.prototxt','/content/face_detector/res10_300x300_ssd_iter_140000.caffemodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEir1j7FF4Y6"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        " \n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        " \n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        " \n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58xAP0edF_HT"
      },
      "source": [
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        " \n",
        "    return image_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDRhz4cqGEFC"
      },
      "source": [
        "import imutils\n",
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "from google.colab.patches import cv2_imshow\n",
        "while True:\n",
        "  js_reply = take_photo(label_html, img_data)\n",
        "  if not js_reply:\n",
        "    break\n",
        "    \n",
        "  image = js_reply_to_image(js_reply)\n",
        " \n",
        "    # grab the frame from the threaded video stream and resize it\n",
        "    # to have a maximum width of 400 pixels\n",
        "  frame = image\n",
        "  v=True\n",
        "  if v == True:\n",
        " \n",
        "    frame = imutils.resize(frame, width=400)\n",
        " \n",
        "    # detect faces in the frame and determine if they are wearing a\n",
        "    # face mask or not\n",
        "    (locs, preds) = detect_and_predict_mask(frame, faceNet, model)\n",
        "    for (box, pred) in zip(locs, preds):\n",
        " \n",
        " \n",
        "        # unpack the bounding box and predictions\n",
        "      (startX, startY, endX, endY) = box\n",
        "      (mask, withoutMask) = pred\n",
        " \n",
        "        # determine the class label and color we'll use to draw\n",
        "        # the bounding box and text\n",
        "      label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "      color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        " \n",
        "        # include the probability in the label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        " \n",
        "        # display the label and bounding box rectangle on the output\n",
        "        # frame\n",
        "      frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      frame=cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        " \n",
        "    # show the output frame\n",
        "      cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_GNwuPEP9_n"
      },
      "source": [
        "changing predicts from 2 to 3 classes. Below code is experimental take on above piece."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFW2pfbVP7nE"
      },
      "source": [
        "import imutils\n",
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "from google.colab.patches import cv2_imshow\n",
        "while True:\n",
        "  js_reply = take_photo(label_html, img_data)\n",
        "  if not js_reply:\n",
        "    break\n",
        "    \n",
        "  image = js_reply_to_image(js_reply)\n",
        " \n",
        "    # grab the frame from the threaded video stream and resize it\n",
        "    # to have a maximum width of 400 pixels\n",
        "  frame = image\n",
        "  v=True\n",
        "  if v == True:\n",
        " \n",
        "    frame = imutils.resize(frame, width=400)\n",
        " \n",
        "    # detect faces in the frame and determine if they are wearing a\n",
        "    # face mask or not\n",
        "    (locs, preds) = detect_and_predict_mask(frame, faceNet, model)\n",
        "    for (box, pred) in zip(locs, preds):\n",
        " \n",
        " \n",
        "        # unpack the bounding box and predictions\n",
        "      (startX, startY, endX, endY) = box\n",
        "      (mask, maskImproper, withoutMask) = pred\n",
        " \n",
        "        # determine the class label and color we'll use to draw\n",
        "        # the bounding box and text\n",
        "      if mask > withoutMask and mask > maskImproper :\n",
        "        label = \"Mask\"\n",
        "      elif maskImproper > withoutMask :\n",
        "        label = \"Improper Mask\"\n",
        "      else :\n",
        "        label = \"No Mask\"\n",
        "      #label = \"Mask\" if mask > withoutMask and mask > maskImproper else \"No Mask\"\n",
        "        \n",
        "      if label == \"Mask\" :\n",
        "        color = (0, 255, 0)\n",
        "      elif label == \"Improper Mask\" :\n",
        "        color = (0, 127, 127)\n",
        "      else :\n",
        "        color = (0, 0, 255)\n",
        "      \n",
        "      #color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        " \n",
        "        # include the probability in the label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, maskImproper, withoutMask) * 100)\n",
        " \n",
        "        # display the label and bounding box rectangle on the output\n",
        "        # frame\n",
        "      frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      frame=cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        " \n",
        "    # show the output frame\n",
        "      cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DauPHus8eUn"
      },
      "source": [
        "**3 class mask detection on local machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cUq7Dsq8ppC"
      },
      "source": [
        "# USAGE\n",
        "# python detect_mask_video.py\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "\t\t(104.0, 177.0, 123.0))\n",
        "\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\n",
        "\t# initialize our list of faces, their corresponding locations,\n",
        "\t# and the list of predictions from our face mask network\n",
        "\tfaces = []\n",
        "\tlocs = []\n",
        "\tpreds = []\n",
        "\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\t# the detection\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t\t# the frame\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\n",
        "\t\t\t# add the face and bounding boxes to their respective\n",
        "\t\t\t# lists\n",
        "\t\t\tfaces.append(face)\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\n",
        "\n",
        "\t# only make a predictions if at least one face was detected\n",
        "\tif len(faces) > 0:\n",
        "\t\t# for faster inference we'll make batch predictions on *all*\n",
        "\t\t# faces at the same time rather than one-by-one predictions\n",
        "\t\t# in the above `for` loop\n",
        "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
        "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "\t# return a 2-tuple of the face locations and their corresponding\n",
        "\t# locations\n",
        "\treturn (locs, preds)\n",
        "\n",
        "# construct the argument parser and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "\tdefault=\"face_detector\",\n",
        "\thelp=\"path to face detector model directory\")\n",
        "ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "\tdefault=\"mask_detector.model\",\n",
        "\thelp=\"path to trained face mask detector model\")\n",
        "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "\thelp=\"minimum probability to filter weak detections\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "maskNet = load_model(args[\"model\"])\n",
        "\n",
        "# initialize the video stream and allow the camera sensor to warm up\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 400 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=400)\n",
        "\n",
        "\t# detect faces in the frame and determine if they are wearing a\n",
        "\t# face mask or not\n",
        "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
        "\n",
        "\t# loop over the detected face locations and their corresponding\n",
        "\t# locations\n",
        "\tfor (box, pred) in zip(locs, preds):\n",
        "\t\t# unpack the bounding box and predictions\n",
        "\t\t(startX, startY, endX, endY) = box\n",
        "\t\t(mask, improperMask, withoutMask) = pred\n",
        "\n",
        "\t\t# determine the class label and color we'll use to draw\n",
        "\t\t# the bounding box and text\n",
        "\t\t#label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\t#color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "    if mask > withoutMask :\n",
        "      if mask > improperMask\n",
        "      \n",
        "\n",
        "\t\t# include the probability in the label\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# display the label and bounding box rectangle on the output\n",
        "\t\t# frame\n",
        "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# show the output frame\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}